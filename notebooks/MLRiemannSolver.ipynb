{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [Abstract](#abs)\n",
    "* [Introduction](#intro)\n",
    "* [Exact Riemann Solver](#ers)\n",
    "    * [Riemann Problem](#ers-rp)\n",
    "    * [Solution of Riemann Problem](#ers-so)\n",
    "    * [Tests](#ers-te)\n",
    "        * [Sod (shock tube) test](#ers-te-so)\n",
    "        * [123 test](#ers-te-123)\n",
    "        * [Left blast wave test](#ers-te-lbw)\n",
    "        * [Right blast wave test](#ers-te-rbw)\n",
    "        * [Two shocks test](#ers-te-tw)\n",
    "    * [Data Visualisation](#ers-dv)\n",
    "* [Machine Learning Powered Riemann Solver](#mlrs)\n",
    "    * [Range of Interest](#mlrs-ri)\n",
    "    * [Data](#mlrs-data)\n",
    "    * [Models](#mlrs-mo)\n",
    "    * [Tests](#mlrs-te)\n",
    "* [Conclusion](#con)\n",
    "* [References](#refer)\n",
    "* [Similar Works](#similar-works)\n",
    "* [Appendix I: Derivation of pressure function for shocks and rarefactions](#app-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract <a class=\"anchor\" id=\"abs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "\n",
    "[Godunov 1959](#ref-godunov-1959) introduced a conservative scheme for numerically solving non-linear systems of hyperbolic conervation laws. His original 1st-order scheme three main steps:\n",
    "* Data reconstruction\n",
    "* Solving local Riemann problem at cell boundaries\n",
    "* Evolving hydro variables\n",
    "\n",
    "Since 1959, higher-order schemes were invented by varius people based on the Godunov scheme (references).\n",
    "\n",
    "Briefly mention higher-order TVD schemes:\n",
    "* [ ] Wighted average flux (WAF) schemes\n",
    "* [ ] MUSCL-Hancock scheme\n",
    "\n",
    "The key ingredient of all these schemes is the solution of local Riemann problem.\n",
    "\n",
    "Briefly mention and reference different Riemann solvers here:\n",
    "* [ ] HLL and HLLC solvers\n",
    "* [ ] Roe solver\n",
    "* [ ] Approximate-state solvers\n",
    "* [ ] Exact solver (more information in a separate chapter)\n",
    "\n",
    "In a typical hydrodynamical simulations, Riemann problem must be solved at all cell boundaries for every time step, thus any improvement in the performance of this task will lead to a significant improvement in overall performance of hydrodynamical simulations.\n",
    "\n",
    "The purpose of this study is to introduce an approximate Machine Learning based Riemann Solver (MLRS hearby) with O(1) complexity to be substitute by current solvers. Our method consist of a multi-layer netwrork trained on a sample of Riemann problem solutions generated by the exact Riemann solver within a wide range of hydrodynamical varibales. Since we are specifically interested in the range of variables relevant for astrophisycal simulations, we limit our scope of interest to only ideal gasses and the solution of the Riemann problem at its initial boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Riemann Solver <a class=\"anchor\" id=\"ers\"></a>\n",
    "\n",
    "Riemann problem does not have a closed-form solution and what we refer here as the exact Riemann solver is an iterative scheme that can be used to calculate the solution of a Riemann problem with a desired accuracy. Here, we are interested in solution of Riemann problem for ideal gasses at the initial boundary of two states (look at [Colella and Glaz 1985](#ref-colella-1985) for a discussion about general Riemann solvers).\n",
    "\n",
    "The exact solution of the Riemann problem is an important reference solution to be used to asses the accuracy and performance of numerical simulations and methods. It is also a good example of a simple intial condition which leads to a rich non-trivial result containing the mathematical and physical essensce of the conservation laws.\n",
    "\n",
    "The solution of the Riemann problem can be seen as the non-linear superposition of solutions of local Riemann problems [Glimm 1965](#ref-glimm-1965).\n",
    "\n",
    "In this chapter we present a procedure to iteratively solve the Riemann problem of inviscid compressible flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riemann Problem <a class=\"anchor\" id=\"ers-rp\"></a>\n",
    "\n",
    "A Riemann problem is an Initial Value Problem (IVP) composed of conservation laws together with intiial piecewise constant data having a discontinuity at the centre.\n",
    "\n",
    "In Cartesian space, conservation laws can be described as,\n",
    "\n",
    "$$\\partial_t U + \\partial_x F(U) = 0$$\n",
    "\n",
    "where $U$ is a vector of conservative variable and $F(U)$ is their fluxes. In three-dimensional space $U$ can be written as,\n",
    "\n",
    "$$U = \\begin{bmatrix} \\rho \\\\ \\rho u \\\\ \\rho v \\\\ \\rho w \\\\ E\\end{bmatrix}$$\n",
    "\n",
    "where $u$, $v$ and $w$ are velocities in $x$, $y$ and $z$ directions and $E$ is the total energy consists of kinetic and internal energies and it equals to $\\frac{1}{2} \\rho \\mathbf{v}^2 + e_{\\text{int}}$. The corresponding flux of the flow in $x$ direction can be written as:\n",
    "\n",
    "$$F_x(U) = \\begin{bmatrix} \\rho u \\\\ \\rho u^2 + p \\\\ \\rho u v \\\\ \\rho u w \\\\ u(E + p) \\end{bmatrix}$$\n",
    "\n",
    "where $p$ is the pressure and can be calculated by the caloric ideal gas Equation of State (EoS):\n",
    "\n",
    "$$e_{\\text{int}} = \\frac{p}{(\\gamma - 1) \\rho}$$\n",
    "\n",
    "The initial piecewise constant condition is also written as,\n",
    "\n",
    "$$U(x,t=0) = U^{(0)}(x) = \\begin{cases}\n",
    "U_L & \\text{if} \\quad x < 0 \\\\\n",
    "U_R & \\text{if} \\quad x > 0\n",
    "\\end{cases}$$\n",
    "\n",
    "Where $U_L$ and $U_R$ are vectors of conservative variables and have constant values over the domain of interest.\n",
    "\n",
    "Here, we assume that the system is strictly hyperbolic, meaning the\n",
    "eigenvalues of the Jacobian matrix $\\mathbf{A} = \\partial_{u_j} f_i$ are\n",
    "real and distinct,\n",
    "\n",
    "$$\\lambda_1 < \\lambda_2 < ... < \\lambda_m$$\n",
    "\n",
    "Each eigenvector corresponds to a wave emerging from the discontinuity and propagating with the velocity equals to its eigenvalue. Generally, the solution of the Riemann problem consists of three types of waves,\n",
    "\n",
    "* one linearly degenerate wave (the contact discontinuity)\n",
    "* (nonlinear) Shock waves\n",
    "* (nonlinear) rarefaction (fan) waves\n",
    "\n",
    "Contact discontinuities are surfaces that separate zones of different density and temperature at fixed pressure. By definition such a surface is in pressure equilibrium, and no gas flows across it. Usually, when the tangential components of velocity on one side differs considerably from that of the gas on the other side, the surface is called a slip discontinuity. The boundary of a supersonic jet and the ambient gas is an example of a slip discontinuity.\n",
    "\n",
    "The two types of nonlinear waves (shocks and rarefactions) arise from abrupt changes in pressure. Shock fronts accompany compression of the medium and rarefactions accompany expansion of the medium.\n",
    "\n",
    "These waves are separating four different states where the conservative vector $U$ acquires from the left to the right the following values, $U_L$, $U_{*L}$, $U_{*R}$ and $U_R$. The symbol $*$ identify points located in the star region between the nonlinear waves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution of Riemann Problem <a class=\"anchor\" id=\"ers-so\"></a>\n",
    "\n",
    "The solution for the pressure at contact discontinuity of the Riemann problem for an ideal gas, $p_*$, is given by the root of the following equation, called the **pressure function**,\n",
    "\n",
    "$$\n",
    "f(p, W_L, W_R) \\equiv f_L(p,W_L) + f_R(p, W_R) + \\Delta u = 0,\n",
    "\\quad \\Delta u \\equiv u_R - u_L\n",
    "$$\n",
    "\n",
    "where \\$f_K$ for $k \\in [L, R]\\$ represents relations across the left and the right nonlinear waves and is given by:\n",
    "\n",
    "$$\n",
    "f_K \\left(p, W_K\\right) = \\begin{cases}\n",
    "  (p - p_K) \\sqrt{(\\frac{A_K}{p + B_K})}\n",
    "  & \\text{if}\\quad p > p_K \\quad \\text{shock} \\\\\n",
    "  & \\\\\n",
    "  \\frac{2 c_{sK}}{\\gamma - 1} \\left[\n",
    "    \\left( \\frac{p}{p_K} \\right)^{\\frac{\\gamma - 1}{2 \\gamma}} - 1\n",
    "  \\right]\n",
    "  & \\text{if}\\quad p < p_K \\quad \\text{rarefaction}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A_K = \\frac{2}{(\\gamma + 1) \\rho_K}, \\quad\n",
    "B_K = \\frac{\\gamma - 1}{\\gamma + 1} p_K\n",
    "$$\n",
    "\n",
    "Following the calculation of $p_*$, the velocity at contact discontinuity, $u_*$, which is a function of the left and right velocities and $p_*$, can be calculated as,\n",
    "\n",
    "$$u_* = \\frac 1 2 (u_L + u_R) + \\frac 1 2 \\left[ f_R(p_*) - f_L(p_*) \\right]$$\n",
    "\n",
    "The remaining unknowns can be found by using standard gas dynamics relations.\n",
    "\n",
    "In order to investigate the behaviour of the pressure function at a given data ($u_L = (\\rho_L, u_L, v_L, w_L, p_L)^T$ and $u_R = (\\rho_R, u_R, v_R, w_R, p_R)^T$), we need to calculate the first and the second derivative of the function w.r.t $p$. The first derivative of the pressure function can be written as,\n",
    "\n",
    "$$\n",
    "f^{\\prime}_K =\n",
    "\\begin{cases}\n",
    "  (\\frac{A_K}{p + B_K})^{1/2}\n",
    "  \\left(\n",
    "    1 - \\frac{p - p_K}{2 (B_k + p)}\n",
    "  \\right) & \\text{if}\\quad p > p_K \\quad \\text{shock} \\\\\n",
    "  & \\\\\n",
    "  \\frac{1}{\\rho_K c_s^K}\n",
    "  \\left(\n",
    "    \\frac{p}{p_K}\n",
    "  \\right)^{\\frac{- (\\gamma + 1)}{2 \\gamma}}\n",
    "  & \\text{if}\\quad p \\lt p_K \\quad \\text{rarefaction}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "as $f^\\prime = f_L^\\prime + f_R^\\prime$ and both $f_{L,R}^\\prime > 0$, we can see that the pressure function is monotonic function.\n",
    "\n",
    "Considering the second derivative of the pressure function\n",
    "\n",
    "$$\n",
    "f^{\\prime\\prime}_K =\n",
    "\\begin{cases}\n",
    "\\frac 1 4 \\left( \\frac{ A_K }{ p + B_K } \\right)^{1/2}\n",
    "\\left[\n",
    "  \\frac{ 4B_K + 3p + p_K }{ ( B_K + p )^2 }\n",
    "\\right]\n",
    "& \\text{if}\\quad p > p_K \\quad \\text{shock} \\\\\n",
    "& \\\\\n",
    "- \\frac{ (\\gamma - 1) c_{sK} }{ 2 \\gamma^2 p_K^2 }\n",
    "\\left(\n",
    "  \\frac{ p }{ p_K }\n",
    "\\right)^{ - ( 3\\gamma + 1 ) / 2\\gamma }\n",
    "& \\text{if}\\quad p \\leq p_K \\quad \\text{rarefaction}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "since $f^{\\prime\\prime} = f_l^{\\prime\\prime} + f_r^{\\prime\\prime}$ and both $f_{L,R}^{\\prime\\prime} < 0$, the pressure function is downward concave.\n",
    "\n",
    "The following figure shows the behaviour of the pressure function in the solution of the Riemann problem,\n",
    "\n",
    "![pressure-function](assets/pressure-function.png)\n",
    "\n",
    "In the above figure, we define,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  & p_{\\text{min}} = \\text{min}(p_L, p_R) \\\\\n",
    "  & p_{\\text{max}} = \\text{max}(p_L, p_R) \\\\\n",
    "  & f_{\\text{min}} = f(p_{\\text{min}}) \\\\\n",
    "  & f_{\\text{max}} = f(p_{\\text{max}})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For a given $p_L$ and $p_R$, it is the velocity differece of two sides, $\\Delta u$, that determines the value of $p_*$.\n",
    "\n",
    "Three distinct intervals, $I_1$, $I_2$ and $I_3$, can be identified in the domain of the pressure function:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  & p_*\\quad\\text{lies in}\\quad I_1 = (0, p_{\\text{min}})\\quad\n",
    "  &\\text{if}\\quad f_{\\text{min}} > 0\\quad\\text{and}\\quad f_{\\text{max}} > 0\\\\\n",
    "  & p_*\\quad\\text{lies in}\\quad I_2 = [p_{\\text{min}}, p_{\\text{max}}]\\quad\n",
    "  &\\text{if}\\quad f_{\\text{min}} \\leq 0\\quad\\text{and}\\quad f_{\\text{max}} \\geq 0\\\\\n",
    "  & p_*\\quad\\text{lies in}\\quad I_3 = (p_{\\text{max}}, \\infty)\\quad\n",
    "  &\\text{if}\\quad f_{\\text{min}} < 0\\quad\\text{and}\\quad f_{\\text{max}} < 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For suficiently large $\\Delta u$ as in $(\\Delta u)_1$, the value of $p_*$ lies in the $I_1$ region and since the root of the pressure function is less than $p_L$ and $p_R$, we end up having two rarefactions. We can follow the same logic to explain $(\\Delta u)_2$ and $(\\Delta u)_3$.\n",
    "\n",
    "Note for a positive solution of star region pressure $p_*$ we require $f(0) < 0$. This leads to the pressure positivity condition,\n",
    "\n",
    "$$\n",
    "(\\Delta u)_{\\text{crit}}\n",
    "  \\equiv \\frac{2 c_{sL}}{\\gamma -1} + \\frac{2 c_{sR}}{\\gamma -1}\n",
    "  > u_R - u_L\n",
    "$$\n",
    "\n",
    "Note that vacuum is created by the nonlinear waves if this condition is violated.\n",
    "\n",
    "Add a new subsection for the solution of Riemann problem in the presense of vacuum:\n",
    "* at the right side\n",
    "* at the left side\n",
    "* at both sides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests <a class=\"anchor\" id=\"ers-te\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sod (shock tube) test <a class=\"anchor\" id=\"ers-te-so\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 123 test <a class=\"anchor\" id=\"ers-te-123\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left blast wave test <a class=\"anchor\" id=\"ers-te-lbw\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rigth blast wave test <a class=\"anchor\" id=\"ers-te-rbw\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two shocks test <a class=\"anchor\" id=\"ers-te-tw\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Powered Riemann Solver <a class=\"anchor\" id=\"mlrs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Range of Interest <a class=\"anchor\" id=\"mlrs-ri\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global packages and variables\n",
    "from ml_riemann_solver import MLRiemannSolver as MLRS\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "import astropy.units as U\n",
    "import astropy.constants as C\n",
    "import csv\n",
    "\n",
    "# !pip install plotly\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "pio.templates.default = \"simple_white\"\n",
    "\n",
    "# Backup directory\n",
    "BACKUP_DIRECTORY = os.environ['HOME'] + '/.mlrs'\n",
    "Path(BACKUP_DIRECTORY).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "rho_min=1e-4 * C.m_p * U.cm**-3\n",
    "rho_max=1e2 * C.m_p * U.cm**-3\n",
    "p_min=1e-16 * U.N / U.m**2\n",
    "p_max=1e-12 * U.N / U.m**2\n",
    "v_min=-150 * U.km / U.s\n",
    "v_max=150 * U.km / U.s\n",
    "\n",
    "def get_filepath(rho_min, rho_max, rho_len, p_min, p_max, p_len, v_min, v_max, v_len, G):\n",
    "    f_rho = f\"{rho_min.si.value:4.3e}-{rho_max.si.value:4.3e}{rho_min.si.unit:FITS}-{rho_len}\"\n",
    "    f_p = f\"{p_min.si.value:4.3e}-{p_max.si.value:4.3e}{p_min.si.unit:FITS}-{p_len}\"\n",
    "    f_v = f\"{v_min.si.value:4.3e}-{v_max.si.value:4.3e}{v_min.si.unit:FITS}-{v_len}\"\n",
    "    \n",
    "    return f\"{BACKUP_DIRECTORY}/{f_rho}__{f_p}__{f_v}_G{G:3.2f}.csv\".replace(' ', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data <a class=\"anchor\" id=\"mlrs-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_p_star(rho_l, v_l, p_l, cs_l, rho_r, v_r, p_r, cs_r, G=5./3.):\n",
    "    G1 = (G - 1.0)/(2.0*G)\n",
    "    G2 = (G + 1.0)/(2.0*G)\n",
    "    G3 = 2.0*G/(G - 1.0)\n",
    "    G4 = 2.0/(G - 1.0)\n",
    "    G5 = 2.0/(G + 1.0)\n",
    "    G6 = (G - 1.0)/(G + 1.0)\n",
    "    G7 = (G - 1.0)/2.0\n",
    "    G8 = G - 1.0\n",
    "    \n",
    "    q_user = 2.0\n",
    "    \n",
    "    cup = 0.25 * (rho_l + rho_r) * (cs_l + cs_r)\n",
    "    ppv = max(0.5 * (p_l + p_r) + 0.5 * (v_l - v_r) * cup, 0.0)\n",
    "    p_min = min(p_l, p_r)\n",
    "    p_max = max(p_l, p_r)\n",
    "    q_max = p_max / p_min\n",
    "\n",
    "    if q_max <= q_user and (p_min <= ppv and ppv <= p_max):\n",
    "        # PVRS Riemann solver\n",
    "        pm = ppv\n",
    "    else:\n",
    "        if ppv <= p_min:\n",
    "            # Two-Rarefaction Riemann solver\n",
    "            pq = (p_l / p_r)**G1\n",
    "            um = (pq * v_l / cs_l + v_r / cs_r + G4 * (pq - 1.0)) / (pq / cs_l + 1.0 / cs_r)\n",
    "            ptl = 1.0 + G7 * (v_l - um) / cs_l\n",
    "            ptr = 1.0 + G7 * (um - v_r) / cs_r\n",
    "            pm = 0.5 * (p_l * ptl**G3 + p_r * ptr**G3)            \n",
    "        else:\n",
    "            # Two-Shock Riemann solver\n",
    "            gel = sqrt((G5 / rho_l) / (G6 * p_l + ppv))\n",
    "            ger = sqrt((G5 / rho_r) / (G6 * p_r + ppv))\n",
    "            pm = (gel * p_l + ger * p_r - (v_r - v_l)) / (gel + ger)\n",
    "    \n",
    "    return pm\n",
    "\n",
    "def wave_function(rho, v, p, p_s, G=5./3.):\n",
    "    if rho <= 0.0 or p <= 0.0:\n",
    "        print('wave function, negative values', rho, v, p, p_s, G)\n",
    "\n",
    "    if p_s > p:\n",
    "        Ak = 2 / ((G+1) * rho)\n",
    "        Bk = (G-1) / (G+1) * p\n",
    "        factor = sqrt(Ak / (Bk + p_s))\n",
    "        f = factor * (p_s - p)p_star = p_prev - (f_l + f_r + (v_r - v_l)) / (fprime_l + fprime_r)\n",
    "        fprime = factor * (1 - (p_s - p) / (2 * (Bk + p_s)))\n",
    "    else:\n",
    "        cs = sqrt(G * p / rho)\n",
    "        f = 2 * cs / (G-1) * ((p_s / p)**((G-1) / (2*G)) - 1)\n",
    "        fprime = 1. / (rho * cs) * (p_s / p)**(-(G+1) / (2*G))\n",
    "\n",
    "    return f, fprime\n",
    "\n",
    "\n",
    "def iterate(rho_l, v_l, p_l, rho_r, v_r, p_r, G=5./3., tolerance=1e-10):\n",
    "    \"\"\"\n",
    "    :param l: left side primitive variables (rho, v, p)\n",
    "    :param r: right side primitive variables (rho, v, p)\n",
    "    \n",
    "    :return p_star\n",
    "    \"\"\"\n",
    "    cs_l = sqrt(G * p_l / rho_l)\n",
    "    cs_r = sqrt(G * p_r / rho_r)\n",
    "    \n",
    "    guessed_p = abs(guess_p_star(rho_l, v_l, p_l, cs_l, rho_r, v_r, p_r, cs_r, G=G))\n",
    "    \n",
    "    found = False\n",
    "\n",
    "    p_star = guessed_p  \n",
    "    p_prev = p_star\n",
    "    \n",
    "    for i in range(10000):\n",
    "        f_l, fprime_l = wave_function(rho_l, v_l, p_l, p_star, G=G)\n",
    "        f_r, fprime_r = wave_function(rho_r, v_r, p_r, p_star, G=G)\n",
    "\n",
    "        p_star = p_prev - (f_l + f_r + (v_r - v_l)) / (fprime_l + fprime_r)\n",
    "\n",
    "        if 2 * abs((p_star - p_prev) / (p_star + p_prev)) < tolerance:\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "#         if p_star < 0:\n",
    "#             p_star = abs(p_star * tolerance)\n",
    "\n",
    "        p_prev = p_star\n",
    "            \n",
    "    if not found:\n",
    "#         print(f'Not found! ({check}) guess: {guessed_p}, p_star: {p_star}, p: ({p_l}, {p_r}), v: ({v_l}, {v_r}), rho: ({rho_l}, {rho_r})')\n",
    "        return None\n",
    "#     else:\n",
    "#         print(f'Found! guess: {guessed_p}, p_star: {p_star}, p: ({p_l}, {p_r}), v: ({v_l}, {v_r}), rho: ({rho_l}, {rho_r})')\n",
    "        \n",
    "    return p_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def write_data(rho_min, rho_max, rho_len, p_min, p_max, p_len, v_min, v_max, v_len, G, rewrite=False):\n",
    "    filepath = get_filepath(rho_min, rho_max, rho_len, p_min, p_max, p_len, v_min, v_max, v_len, G)\n",
    "    print(filepath)\n",
    "\n",
    "    if Path(filepath).is_file() and not rewrite:\n",
    "        print(\"CSV file exist\")\n",
    "        return\n",
    "    \n",
    "    ranges = {\n",
    "        'rho': np.logspace(np.log10(rho_min.si.value), np.log10(rho_max.si.value), rho_len),\n",
    "        'p': np.logspace(np.log10(p_min.si.value), np.log10(p_max.si.value), p_len,),\n",
    "        'v': np.linspace(v_min.si.value, v_max.si.value, v_len,),\n",
    "    }\n",
    "    \n",
    "    fileds = [\n",
    "        f'rho_l [{rho_min.si.unit:FITS}]', f'v_l [{v_min.si.unit:FITS}]', f'p_l [{p_min.si.unit:FITS}]',\n",
    "        f'rho_r [{rho_min.si.unit:FITS}]', f'v_r [{v_min.si.unit:FITS}]', f'p_r [{p_min.si.unit:FITS}]',\n",
    "        f'p_star [{p_min.si.unit:FITS}]'\n",
    "    ]\n",
    "    \n",
    "    with open(filepath, 'w') as csvfile:  \n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(fileds)\n",
    "        \n",
    "        print('Start...')\n",
    "        \n",
    "        total = rho_len**2 * v_len**2 * p_len**2\n",
    "        cntr = 0\n",
    "        percent = 0\n",
    "        \n",
    "        print(f'{percent/10000.0:7.4f}%', end='\\r')\n",
    "        \n",
    "        for rho_l in ranges['rho']:\n",
    "            for rho_r in ranges['rho']:\n",
    "                for p_l in ranges['p']:\n",
    "                    for p_r in ranges['p']:\n",
    "                        for v_l in ranges['v']:\n",
    "                            for v_r in ranges['v']:\n",
    "                                cs_l = sqrt(G * p_l / rho_l)\n",
    "                                cs_r = sqrt(G * p_r / rho_r)\n",
    "                                \n",
    "                                # pressure positivity test\n",
    "                                if (2. / (G - 1)) * (cs_l + cs_r) <= (v_r - v_l):\n",
    "                                    p_star = 0.0\n",
    "                                else:\n",
    "                                    p_star = iterate(rho_l, v_l, p_l, rho_r, v_r, p_r, G=G)\n",
    "                                \n",
    "                                if p_star is not None and p_star > 0:\n",
    "                                    csvwriter.writerow([rho_l, v_l, p_l, rho_r, v_r, p_r, p_star])\n",
    "                                \n",
    "                                cntr += 1\n",
    "                                new_percent = int(cntr / total * 1000000)\n",
    "                                if new_percent > percent:\n",
    "                                    percent = new_percent\n",
    "                                    print(f'{percent/10000.0:7.4f}%', end='\\r')\n",
    "    \n",
    "# write_data(\n",
    "#     rho_min=rho_min, rho_max=rho_max, rho_len=10,\n",
    "#     p_min=p_min, p_max=p_max, p_len=10,\n",
    "#     v_min=v_min, v_max=v_max, v_len=10,\n",
    "#     G=5./3.,\n",
    "#     rewrite=True\n",
    "# )\n",
    "\n",
    "write_data(\n",
    "    p_min=0.001 * U.N / U.m**2, p_max=1000 * U.N / U.m**2, p_len=7,\n",
    "    v_min=-1 * U.m / U.s, v_max=1 * U.m / U.s, v_len=5,\n",
    "    rho_min=0.01 * U.kg / U.m**2, rho_max=100 * U.kg / U.m**2, rho_len=5,\n",
    "    G=5.0/3.0,\n",
    "    rewrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation <a class=\"anchor\" id=\"ers-dv\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(rho_min, rho_max, rho_len, p_min, p_max, p_len, v_min, v_max, v_len, G, canvas_size=(256, 256)):\n",
    "    fpath = get_filepath(\n",
    "        rho_min=1e-4 * C.m_p * U.cm**-3, rho_max=1e2 * C.m_p * U.cm**-3, rho_len=10,\n",
    "        p_min=1e-16 * U.N / U.m**2, p_max=1e-12 * U.N / U.m**2, p_len=10,\n",
    "        v_min=-150 * U.km / U.s, v_max=150 * U.km / U.s, v_len=10,\n",
    "        G=5./3.,\n",
    "    )\n",
    "    \n",
    "    if not Path(fpath).is_file():\n",
    "        print('Not found!')\n",
    "        return\n",
    "    \n",
    "    rcanvas = np.zeros(canvas_size)\n",
    "    rcounts = np.zeros(canvas_size, dtype=np.int)\n",
    "    vcanvas = np.zeros(canvas_size)\n",
    "    vcounts = np.zeros(canvas_size, dtype=np.int)\n",
    "    pcanvas = np.zeros(canvas_size)\n",
    "    pcounts = np.zeros(canvas_size, dtype=np.int)\n",
    "    \n",
    "    f = open(fpath, 'r')\n",
    "    csv_reader = csv.reader(f)\n",
    "    next(csv_reader)\n",
    "    \n",
    "    rmin, rmax = rho_min.si.value, rho_max.si.value\n",
    "    pmin, pmax = p_min.si.value, p_max.si.value\n",
    "    vmin, vmax = v_min.si.value, v_max.si.value\n",
    "\n",
    "    for irow, row in enumerate(csv_reader):\n",
    "        rl, vl, pl = float(row[0]), float(row[1]), float(row[2])\n",
    "        rr, vr, pr = float(row[3]), float(row[4]), float(row[5])\n",
    "        p_star = float(row[6])\n",
    "        \n",
    "        if p_star is None:\n",
    "            print('p_star is none')\n",
    "            continue\n",
    "        \n",
    "        ir = min(max(0, int((np.log10(rl/rmin)) / (np.log10(rmax/rmin)) * canvas_size[0])), canvas_size[0]-1)\n",
    "        jr = min(max(0, int((np.log10(rr/rmin)) / (np.log10(rmax/rmin)) * canvas_size[1])), canvas_size[1]-1)\n",
    "        \n",
    "        iv = min(max(0, int((vl-vmin) / (vmax-vmin) * canvas_size[0])), canvas_size[0]-1)\n",
    "        jv = min(max(0, int((vr-vmin) / (vmax-vmin) * canvas_size[1])), canvas_size[1]-1)\n",
    "        \n",
    "        ip = min(max(0, int((np.log10(pl/pmin)) / (np.log10(pmax/pmin)) * canvas_size[0])), canvas_size[0]-1)\n",
    "        jp = min(max(0, int((np.log10(pr/pmin)) / (np.log10(pmax/pmin)) * canvas_size[1])), canvas_size[1]-1)\n",
    "\n",
    "        rcanvas[ir, jr] = (rcounts[ir, jr] * rcanvas[ir, jr] + p_star) / (rcounts[ir, jr] + 1)\n",
    "        rcounts[ir, jr] += 1\n",
    "        \n",
    "        vcanvas[iv, jv] = (vcounts[iv, jv] * vcanvas[iv, jv] + p_star) / (vcounts[iv, jv] + 1)\n",
    "        vcounts[iv, jv] += 1\n",
    "        \n",
    "        pcanvas[ip, jp] = (pcounts[ip, jp] * pcanvas[ip, jp] + p_star) / (pcounts[ip, jp] + 1)\n",
    "        pcounts[ip, jp] += 1\n",
    "\n",
    "    f.close()\n",
    "        \n",
    "    return rcanvas, rcounts, vcanvas, vcounts, pcanvas, pcounts\n",
    "        \n",
    "rcanvas, rcounts, vcanvas, vcounts, pcanvas, pcounts = visualize_data(\n",
    "    rho_min=rho_min, rho_max=rho_max, rho_len=10,\n",
    "    p_min=p_min, p_max=p_max, p_len=10,\n",
    "    v_min=v_min, v_max=v_max, v_len=10,\n",
    "    G=5./3.,\n",
    "    canvas_size=(10, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_heatmap(data, title):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Heatmap(\n",
    "        z=data, colorscale='Spectral', reversescale=True,\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=title, width=500, height=500,\n",
    "        xaxis=dict(mirror=True),\n",
    "        yaxis=dict(mirror=True),\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "plot_heatmap(np.log10(rcanvas), 'rho')\n",
    "plot_heatmap(np.log10(vcanvas), 'v')\n",
    "plot_heatmap(np.log10(pcanvas), 'p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models <a class=\"anchor\" id=\"mlrs-mo\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from torch import from_numpy, optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10\n",
    "\n",
    "def load_data(path):\n",
    "    if not Path(path).is_file():\n",
    "        print(\"File does not exist!\")\n",
    "        return None\n",
    "    \n",
    "    result = np.genfromtxt(path, skip_header=1, delimiter=',', converters={\n",
    "        0: lambda s: float(s),\n",
    "        1: lambda s: float(s),\n",
    "        2: lambda s: float(s),\n",
    "        3: lambda s: float(s),\n",
    "        4: lambda s: float(s),\n",
    "        5: lambda s: float(s),\n",
    "        6: lambda s: float(s),\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "fpath = get_filepath(\n",
    "    rho_min=rho_min, rho_max=rho_max, rho_len=10,\n",
    "    p_min=p_min, p_max=p_max, p_len=10,\n",
    "    v_min=v_min, v_max=v_max, v_len=10,\n",
    "    G=5./3.,\n",
    ")\n",
    "\n",
    "M = load_data(fpath)\n",
    "M = M[np.isfinite(M).all(axis=1)]\n",
    "M = M[M[:, 6] > 0]\n",
    "\n",
    "print(len(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((len(M), 6))\n",
    "filt = np.zeros((len(M), 6))\n",
    "\n",
    "drho = (np.log10(rho_max.si.value * rho_min.si.value)) / 2\n",
    "dp = np.log10(p_max.si.value * p_min.si.value) / 2\n",
    "\n",
    "normalized='norm'\n",
    "data[:, 0] = (np.log10(M[:, 0]) - drho) / drho\n",
    "data[:, 1] = (np.log10(M[:, 2]) - dp) / dp\n",
    "data[:, 2] = (np.log10(M[:, 3]) - drho) / drho\n",
    "data[:, 3] = (np.log10(M[:, 5]) - dp) / dp\n",
    "data[:, 4] = (M[:, 4] - M[:, 1]) / (2 * v_max.si.value)\n",
    "data[:, 5] = (np.log10(M[:, 6]) - dp) / dp\n",
    "\n",
    "# normalized='not_norm'\n",
    "# data[:, 0] = M[:, 0]\n",
    "# data[:, 1] = M[:, 2]\n",
    "# data[:, 2] = M[:, 3]\n",
    "# data[:, 3] = M[:, 5]\n",
    "# data[:, 4] = M[:, 4] - M[:, 1]\n",
    "# data[:, 5] = M[:, 6]\n",
    "\n",
    "data = data[~np.isnan(data).any(axis=1) ]\n",
    "\n",
    "filt[:, 0] = M[:, 0]\n",
    "filt[:, 1] = M[:, 2]\n",
    "filt[:, 2] = M[:, 3]\n",
    "filt[:, 3] = M[:, 5]\n",
    "filt[:, 4] = M[:, 4] - M[:, 1]\n",
    "filt[:, 5] = M[:, 6]\n",
    "\n",
    "mask = (filt[:, 0] > 0) & (filt[:, 1] > 0) & (filt[:, 2] > 0) & (filt[:, 3] > 0) & (filt[:, 5] > 0)\n",
    "\n",
    "data = data[mask]\n",
    "\n",
    "dataset = from_numpy(data)\n",
    "print(f\"Dataset shape: {dataset.size()}\")\n",
    "print(f\"first row:  {[float(x) for x in dataset[0]]}\")\n",
    "print(f\"middle row: {[float(x) for x in dataset[int(len(dataset) / 2)]]}\")\n",
    "print(f\"last row:   {[float(x) for x in dataset[-1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class RSModel(nn.Module): \n",
    "    ACTIVATION = {\n",
    "        'ReLu': nn.ReLU(),\n",
    "        'LogSigmoid': nn.LogSigmoid(),\n",
    "        'Sigmoid': nn.Sigmoid(),\n",
    "        'Tanh': nn.Tanh(),\n",
    "        'LeakyReLu': nn.LeakyReLU(),\n",
    "        'Softsign': nn.Softsign(),\n",
    "    }\n",
    "\n",
    "    LOSS = {\n",
    "        'L1': nn.L1Loss(),\n",
    "        'MSE': nn.MSELoss(),\n",
    "        'KLDiv': nn.KLDivLoss(reduction = 'batchmean'),\n",
    "    }\n",
    "\n",
    "    OPTIMIZER = {\n",
    "        'Adadelta': lambda x, y: optim.Adadelta(x, lr=y, rho=0.9, eps=1e-06, weight_decay=0),\n",
    "        'Adagrad': lambda x, y: optim.Adagrad(x, lr=y, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10),\n",
    "        'Adam': lambda x, y: optim.Adam(x, lr=y),\n",
    "        'AdamW': lambda x, y: optim.AdamW(x, lr=y, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False),\n",
    "        'LBFGS': lambda x, y: optim.LBFGS(x, lr=y, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=100, line_search_fn=None),\n",
    "        'RMSprop': lambda x, y: optim.RMSprop(x, lr=y, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False),\n",
    "        'Rprop': lambda x, y: optim.Rprop(x, lr=y, etas=(0.5, 1.2), step_sizes=(1e-06, 50)),\n",
    "        'SGD': lambda x, y: optim.SGD(x, lr=y, momentum=0.9),\n",
    "    }\n",
    "    \n",
    "    def __init__(self, arch, loss, optimizer, learning_rate):\n",
    "        \"\"\"\n",
    "        arch: [(5, 128), 'ReLu', (128, 128), 'Sigmoid', (128, 1)]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.log = []\n",
    "        \n",
    "        self.arch = arch\n",
    "        self.arch_str = '-'.join([str(x) for x in arch]).replace(' ', '').replace('(', '').replace(')', '').replace(',', '_')\n",
    "        self.logging(f\"Arch: {self.arch_str}\")\n",
    "        \n",
    "        self.layer_sizes = [x[0] for x in arch if isinstance(x, tuple)] + [arch[-1][1]]\n",
    "        self.activations_labels = [x for x in arch if isinstance(x, str)]\n",
    "        \n",
    "        nlayer = 0\n",
    "        self.layers = []\n",
    "        for l in self.arch:\n",
    "            if isinstance(l, tuple):\n",
    "                nlayer += 1\n",
    "                self.layers.append((f\"L{nlayer}\", nn.Linear(l[0], l[1])))\n",
    "            elif isinstance(l, str):\n",
    "                self.layers.append((l, self.ACTIVATION[l]))\n",
    "                \n",
    "        self.model = nn.Sequential(OrderedDict(self.layers))\n",
    "            \n",
    "        self.loss_name = loss\n",
    "        self.logging(f\"Loss function: {loss}\")\n",
    "        self.loss = self.LOSS[loss]\n",
    "            \n",
    "        self.optimizer_name = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.logging(f\"Optimizaer: {optimizer}, learning rate: {learning_rate}\")\n",
    "        \n",
    "        self.losses = []\n",
    "        self.loss_slopes = []\n",
    "        \n",
    "    def logging(self, msg):\n",
    "        self.log.append(msg)\n",
    "        print(msg)\n",
    "        \n",
    "    def load(self, dataset, validation_fraction, batch_size):\n",
    "        val_size = int(validation_fraction * len(dataset))\n",
    "        train_size = len(dataset) - val_size\n",
    "        \n",
    "        self.logging(f\"Loading: training size: {train_size} validation size: {val_size}\")\n",
    "        \n",
    "        self.train_ds, self.val_ds = random_split(dataset, [train_size, val_size])\n",
    "        \n",
    "        self.train_loader = DataLoader(self.train_ds, batch_size, shuffle=True)\n",
    "        self.val_loader = DataLoader(self.val_ds, batch_size) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "        \n",
    "    def train(self, n_epochs, break_criterion=0.01):\n",
    "        optimizer = self.OPTIMIZER[self.optimizer_name](self.parameters(), self.learning_rate)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            losses = 0\n",
    "            for i, batch in enumerate(self.train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                inputs = batch[:, :self.layer_sizes[0]].type(torch.FloatTensor)\n",
    "                expected = batch[:, -self.layer_sizes[-1]].type(torch.FloatTensor)\n",
    "                \n",
    "                outputs = self(inputs)\n",
    "                loss = self.loss(torch.squeeze(outputs), expected)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                losses += loss.item()\n",
    "            \n",
    "            self.losses.append(losses / len(self.train_loader))\n",
    "            print(f\"Training ({epoch + 1} / {n_epochs}): loss = {self.losses[-1]:6.4e}\")\n",
    "            \n",
    "    def validation(self, conv_outputs, conv_expected, tolerance, do_print=(False, 5)):\n",
    "        with torch.no_grad():\n",
    "            n_corrects, n_total = 0, 0\n",
    "            \n",
    "            for i, batch in enumerate(self.val_loader):\n",
    "                inputs = torch.squeeze(batch[:, :self.layer_sizes[0]].type(torch.FloatTensor))\n",
    "                expected = torch.squeeze(batch[:, -self.layer_sizes[-1]].type(torch.FloatTensor))\n",
    "                \n",
    "                outputs = self(inputs)\n",
    "                \n",
    "                outputs = conv_outputs(torch.squeeze(outputs))\n",
    "                expected = conv_expected(torch.squeeze(expected))\n",
    "                \n",
    "                n_total += len(outputs)\n",
    "                n_corrects += (\n",
    "                    np.abs((outputs - expected) / expected) < tolerance\n",
    "                ).sum().item()\n",
    "                \n",
    "                \n",
    "                if do_print[0] and i % int((len(self.val_loader) - 1) / do_print[1]) == 0:\n",
    "                    print(f\"{i:6d} - inputs: {inputs[0]}\")\n",
    "                    print(f\"       - output: {outputs[0]}\")\n",
    "                    print(f\"       - expected: {expected[0]}\")\n",
    "                    print(f\"       - diff: {abs((outputs[0] - expected[0])/expected[0])}\")\n",
    "                \n",
    "        accuracy = n_corrects / n_total\n",
    "        self.logging(f\"Validation: accuracy = {accuracy * 100:6.4e}%\")\n",
    "        return accuracy\n",
    "    \n",
    "    def id(self):\n",
    "        return f\"{self.arch_str}-{self.loss_name}-{self.optimizer_name}-{str(self.learning_rate).replace('.', '_')}\"\n",
    "    \n",
    "    def image_path(self, prefix):\n",
    "        return f\"{prefix}/{self.id()}.png\"\n",
    "    \n",
    "    def save_losses(self, prefix='.'):\n",
    "        csv_path = f'{prefix}/{self.id()}.csv'\n",
    "        \n",
    "        with open(csv_path, 'w') as f:\n",
    "            for l in self.losses:\n",
    "                f.write(str(l) + '\\n')\n",
    "                \n",
    "        self.logging(f\"Saved the losses into {csv_path}\")\n",
    "            \n",
    "    def plot(self, y_range, width=500, height=500, prefix='.'):\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(range(len(self.losses))), y=self.losses\n",
    "        ))\n",
    "        \n",
    "        for i, msg in enumerate(self.log):\n",
    "            fig.add_annotation(\n",
    "                xref='paper', yref='paper', xanchor='left', yanchor='top',\n",
    "                x=0.05, y=-0.2 -i * 0.06,\n",
    "                showarrow=False, text=msg,\n",
    "            )\n",
    "            \n",
    "        caption_height = (0.2 + (len(self.log) + 1) * 0.06) * height\n",
    "        \n",
    "        fig.update_layout(\n",
    "            width=width, height=caption_height + height, margin=dict(t=20, r=20, b=20 + caption_height, l=20),\n",
    "            xaxis=dict(mirror=True, type='linear', title='# of epoch'),\n",
    "            yaxis=dict(mirror=True, type='log', title=self.loss_name, range=y_range),\n",
    "        )\n",
    "        \n",
    "        fig.write_image(self.image_path(prefix), width=width, height=caption_height + height, scale=3)\n",
    "        \n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(pio.orca.status)\n",
    "\n",
    "\n",
    "def brute_force(\n",
    "    niter=32,\n",
    "    v1=lambda x: 10**(dp * x + dp),\n",
    "    v2=lambda x: 10**(dp * x + dp),\n",
    "    vrange=1e-6,\n",
    "    yrange=(-6, -1.5)):\n",
    "    prefix = f'/Users/Saeed/Desktop/models.{normalized}'\n",
    "\n",
    "    for act_1 in ['ReLu']: # , 'Tanh']:\n",
    "#         for act_2 in ['ReLu', 'Tanh']:\n",
    "        for act_3 in ['Sigmoid', 'Tanh']:\n",
    "            for loss in ['MSE',]:\n",
    "                for lr in [0.01, 0.001, 0.0001, 0.00001]:\n",
    "                    for opt in ['Adam', 'SGD']:#, 'AdamW', 'Rprop']:\n",
    "#                         model = RSModel([(5, 19), act_1, (19, 67), act_2, (67, 128), act_3, (128, 1)], loss, opt, lr)\n",
    "                        model = RSModel([(5, 256), act_1, (256, 128), act_3, (128, 1)], loss, opt, lr)\n",
    "                        image_path = model.image_path(prefix)\n",
    "                        saved_model_path = f\"{prefix}/{model.id()}.pt\"\n",
    "\n",
    "                        if Path(image_path).is_file():\n",
    "                            print(f\"File exists: {image_path}\")\n",
    "                            del model\n",
    "                            continue\n",
    "\n",
    "                        if Path(saved_model_path).is_file():\n",
    "                            print(f\"Model is already saved: {saved_model_path}\")\n",
    "                            del model\n",
    "                            continue\n",
    "\n",
    "                        model.load(dataset, 0.05, 128)\n",
    "                        model.train(niter)\n",
    "                        model.validation(v1, v2, vrange, do_print=(True, 5))\n",
    "                        model.save_losses(prefix=prefix)\n",
    "                        torch.save(model.state_dict(), saved_model_path)\n",
    "                        model.plot(y_range=yrange, prefix=prefix)\n",
    "                        del model\n",
    "                            \n",
    "def long_brute_force(\n",
    "    niter=1024,\n",
    "    v1=lambda x: 10**(dp * x + dp),\n",
    "    v2=lambda x: 10**(dp * x + dp),\n",
    "    vrange=1e-6,\n",
    "    yrange=(-8, -2)):\n",
    "    prefix = f'/Users/Saeed/Desktop/long_run_models.{normalized}'\n",
    "    \n",
    "    for act in ['Tanh', 'Sigmoid']:\n",
    "        for n in [32, 64, 128, 256, 512]:\n",
    "            model = RSModel([(5, n), 'ReLu', (n, n), act, (n, 1)], 'MSE', 'Adam', 0.001)\n",
    "            image_path = model.image_path(prefix)\n",
    "            saved_model_path = f\"{prefix}/{model.id()}.pt\"\n",
    "            \n",
    "            if Path(image_path).is_file():\n",
    "                print(f\"Model exists: {image_path}\")\n",
    "                del model\n",
    "                continue\n",
    "                \n",
    "            if Path(saved_model_path).is_file():\n",
    "                print(f\"Model is already saved: {saved_model_path}\")\n",
    "                del model\n",
    "                continue\n",
    "            \n",
    "            model.load(dataset, 0.05, 128)\n",
    "            model.train(niter)\n",
    "            model.validation(v1, v2, vrange)\n",
    "            \n",
    "            try:\n",
    "                model.save_losses(prefix=prefix)\n",
    "            except:\n",
    "                time.sleep(10)\n",
    "                try:\n",
    "                    model.save_losses(prefix=prefix)\n",
    "                except:\n",
    "                    print(f\"save_losses failed\")\n",
    "                \n",
    "            try:\n",
    "                torch.save(model.state_dict(), saved_model_path)\n",
    "            except:\n",
    "                time.sleep(10)\n",
    "                try:\n",
    "                    torch.save(model.state_dict(), saved_model_path)\n",
    "                except:\n",
    "                    print(f\"model save failed\")\n",
    "            \n",
    "            try:\n",
    "                model.plot(y_range=yrange, prefix=prefix)\n",
    "            except:\n",
    "                try:\n",
    "                    model.plot(y_range=yrange, prefix=prefix)\n",
    "                except:\n",
    "                    print(f\"plotly failed\")\n",
    "            \n",
    "            del model\n",
    "            \n",
    "def single_learning(\n",
    "    niter=10240,\n",
    "    lr=0.001,\n",
    "    v1=lambda x: 10**(dp * x + dp),\n",
    "    v2=lambda x: 10**(dp * x + dp),\n",
    "    vrange=1e-6):\n",
    "    \n",
    "    prefix = f'/Users/Saeed/Desktop/single_long_run_models.{normalized}'\n",
    "    \n",
    "    n = 128\n",
    "    \n",
    "    model = RSModel([(5, n), 'ReLu', (n, n), 'Sigmoid', (n, 1)], 'MSE', 'Adam', lr)\n",
    "    image_path = model.image_path(prefix)\n",
    "    saved_model_path = f\"{prefix}/{model.id()}.pt\"\n",
    "\n",
    "    if Path(image_path).is_file():\n",
    "        print(f\"Model exists: {image_path}\")\n",
    "        del model\n",
    "        return\n",
    "\n",
    "    if Path(saved_model_path).is_file():\n",
    "        print(f\"Model is already saved: {saved_model_path}\")\n",
    "        del model\n",
    "        return\n",
    "\n",
    "    model.load(dataset, 0.05, 128)\n",
    "    model.train(niter)\n",
    "    model.validation(v1, v2, vrange)\n",
    "\n",
    "    try:\n",
    "        model.save_losses(prefix=prefix)\n",
    "    except:\n",
    "        time.sleep(10)\n",
    "        try:\n",
    "            model.save_losses(prefix=prefix)\n",
    "        except:\n",
    "            print(f\"save_losses failed\")\n",
    "\n",
    "    try:\n",
    "        torch.save(model.state_dict(), saved_model_path)\n",
    "    except:\n",
    "        time.sleep(10)\n",
    "        try:\n",
    "            torch.save(model.state_dict(), saved_model_path)\n",
    "        except:\n",
    "            print(f\"model save failed\")\n",
    "\n",
    "    try:\n",
    "        model.plot(y_range=(-8, -2), prefix=prefix)\n",
    "    except:\n",
    "        try:\n",
    "            model.plot(y_range=(-8, -2), prefix=prefix)\n",
    "        except:\n",
    "            print(f\"plotly failed\")\n",
    "\n",
    "    del model\n",
    "\n",
    "# brute_force(\n",
    "#     niter=128,\n",
    "#     v1=lambda x: 10**(dp * x + dp),\n",
    "#     v2=lambda x: 10**(dp * x + dp),\n",
    "#     vrange=1e-6,\n",
    "#     yrange=(-6, -1.5)\n",
    "# )\n",
    "# single_learning(niter=10240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_check(path):\n",
    "    n = 128\n",
    "    lr = 0.001\n",
    "    \n",
    "    try:\n",
    "        model = RSModel([(5, n), 'ReLu', (n, n), 'Sigmoid', (n, 1)], 'MSE', 'Adam', lr)\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        model.load(dataset, 0.05, 128)\n",
    "    except:\n",
    "        print(f\"Cannot load the model! {path}\")\n",
    "        return\n",
    "    \n",
    "    v1=lambda x: 10**(dp * x + dp)\n",
    "    v2=lambda x: 10**(dp * x + dp)\n",
    "    vrange=0.05\n",
    "    \n",
    "    model.validation(v1, v2, vrange)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def compare_model_with_fortran(path):\n",
    "    n = 128\n",
    "    lr = 0.001\n",
    "    inp = [-0.183694527, -0.293414205, -0.183694527, -0.293414205, 0.0]\n",
    "    \n",
    "    try:\n",
    "        model = RSModel([(5, n), 'ReLu', (n, n), 'Sigmoid', (n, 1)], 'MSE', 'Adam', lr)\n",
    "        model_params = torch.load(path)\n",
    "        model.load_state_dict(model_params)\n",
    "        model.load(dataset, 0.05, 128)\n",
    "    except:\n",
    "        print(f\"Cannot load the model! {path}\")\n",
    "        return\n",
    "    \n",
    "    print('inaj')\n",
    "    def relu(x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    w, b = [], []\n",
    "    for i in range(3):\n",
    "        wtag = f\"model.L{i+1}.weight\"\n",
    "        btag = f\"model.L{i+1}.bias\"\n",
    "        \n",
    "        w.append(model_params[wtag].numpy().transpose())\n",
    "        b.append(model_params[btag].numpy().transpose())\n",
    "        \n",
    "    print('w0', w[0][0, :5])\n",
    "    print('b0', b[0][:5])\n",
    "    print('w1', w[1][0, :5])\n",
    "    print('b1', b[1][:5])\n",
    "    print('w2', w[2][:15, 0])\n",
    "    print('b2', b[2][:1])\n",
    "    \n",
    "    mod = model(torch.FloatTensor(inp))\n",
    "    fort = np.matmul(sigmoid(np.matmul(relu(np.matmul(inp, w[0]) + b[0]), w[1]) + b[1]), w[2]) + b[2]\n",
    "    \n",
    "    print(mod.detach().numpy()[0], fort[0])\n",
    "\n",
    "# m = load_and_check(\n",
    "#     '/Users/Saeed/Desktop/single_long_run_models.norm.done/5_128-ReLu-128_128-Sigmoid-128_1-MSE-Adam-0_001.pt',\n",
    "# )\n",
    "\n",
    "compare_model_with_fortran(\n",
    "    '/Users/Saeed/Desktop/single_long_run_models.norm.done/5_128-ReLu-128_128-Sigmoid-128_1-MSE-Adam-0_001.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_to_rhyme(path):\n",
    "    h5path = f\"{path}.h5\"\n",
    "    \n",
    "    if Path(h5path).is_file():\n",
    "        print(f\"File exists! {h5path}\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        model = torch.load(path)\n",
    "    except:\n",
    "        print('Cannot load the model!')\n",
    "        return\n",
    "    \n",
    "    f = h5py.File(h5path, \"w\")\n",
    "    \n",
    "    f.attrs['n_layers'] = int(len(model.keys()) / 2)\n",
    "    \n",
    "    f.attrs['model_unit_system'] = 'SI'\n",
    "    \n",
    "    f.attrs['d_norm'] = \"(log10(rho_0) - drho) / drho\"\n",
    "    f.attrs['drho'] = drho\n",
    "    \n",
    "    f.attrs['p_norm'] = \"(log10(p_0) - dp) / dp\"\n",
    "    f.attrs['dp'] = dp\n",
    "    \n",
    "    f.attrs['v_norm'] = \"(v_r - v_l) / dv\"\n",
    "    f.attrs['dv'] = 2 * v_max.si.value\n",
    "    \n",
    "    w, b = [], []\n",
    "    for i in range(3):\n",
    "        model_params = torch.load(path)\n",
    "        \n",
    "        wtag = f\"model.L{i+1}.weight\"\n",
    "        btag = f\"model.L{i+1}.bias\"\n",
    "        \n",
    "        w.append(np.asfortranarray(model_params[wtag].numpy().transpose()))\n",
    "        b.append(np.asfortranarray(model_params[btag].numpy().transpose()))\n",
    "    \n",
    "    for i in range(f.attrs['n_layers']):\n",
    "        lay = f.create_group(f'layer_{i}')\n",
    "        \n",
    "        lay.attrs[f'weights_shape'] = w[i].shape\n",
    "        lay.attrs[f'biases_shape'] = b[i].shape\n",
    "        print('w', w[i].shape)\n",
    "        print('b', b[i].shape)\n",
    "        \n",
    "        wdata = lay.create_dataset('weights', w[i].shape, dtype=np.float32)\n",
    "        wdata[:, :] = w[i]\n",
    "        \n",
    "        bdata = lay.create_dataset('biases', [1, b[i].shape[0]], dtype=np.float32)\n",
    "        bdata[0, :] = b[i]\n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "torch_to_rhyme('/Users/Saeed/Desktop/single_long_run_models.norm.done/5_128-ReLu-128_128-Sigmoid-128_1-MSE-Adam-0_001.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests <a class=\"anchor\" id=\"mlrs-te\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion <a class=\"anchor\" id=\"con\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References <a class=\"anchor\" id=\"refer\"></a>\n",
    "* [A Finite Difference Method for the Computation of Discontinuous Solutions of the Equations of Fluid Dynamics](http://www.mathnet.ru/links/ece056bd10695ed71d90b68f97f15e20/sm4873.pdf) by S. K. Godunov [1959] <a class=\"anchor\" id=\"ref-godunov-1959\"></a>\n",
    "* [Solution in the Large for Nonlinear Hyperbolic Systems of Equations](https://onlinelibrary.wiley.com/doi/epdf/10.1002/cpa.3160180408) by J. Glimm [1965] <a class=\"anchor\" id=\"ref-glimm-1965\"></a>\n",
    "* [Efficient Solution Algorithms for the Riemann Problem for Real Gases](https://www.sciencedirect.com/science/article/pii/0021999185901469) by P. Colella and H. H. Glaz [1985] <a class=\"anchor\" id=\"ref-colella-1985\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar works <a class=\"anchor\" id=\"similar-works\"></a>\n",
    "* [Constraint-Aware Neural Networks for RiemannProblems](file:///Users/Saeed/Downloads/ML_Riemann_2019.pdf) by Magiera et al. [2019]\n",
    "* [Machine learning approaches for the solution of the Riemann problem in fluid dynamics: a case study](file:///Users/Saeed/Downloads/ML_QE_RP.pdf) by Gyrya et al. [2019]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix I: Derivation of pressure function for shocks and rarefactions <a class=\"anchor\" id=\"app-1\"></a>\n",
    "\n",
    "In order to derive the pressure function for shock waves we move to a frame of reference moving at the shock velocity and we introduce the relative velocities as,\n",
    "\n",
    "$$\\hat{u}_K = u_K - S_K, \\quad \\hat{u}_* = u_* - S_K$$\n",
    "\n",
    "By substituting Rankine-Hugoniot conditions in to $f_K$ equation we can derive a relation for shock waves as,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\rho_K \\hat{u}_K &= \\rho_* \\hat{u}_* \\\\\n",
    "  \\rho_K \\hat{u}_K^2 + p_K &= \\rho_* \\hat{u}_*^2 + p_* \\\\\n",
    "  \\hat{u}_K (\\hat{E}_K + p_K) &= \\hat{u}_* (\\hat{E}_* + p_*)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* [ ] Substitution?\n",
    "\n",
    "In the case of a rarefaction wave, $f_K$ can be derived by assuming the isentropic relation\n",
    "\n",
    "$$p = C \\rho^{\\gamma}, \\quad C = p_K / \\rho_K^{\\gamma}$$\n",
    "\n",
    "from which we write,\n",
    "\n",
    "$$\\rho_{*K} = \\rho_K \\left( \\frac{p_*}{p_K} \\right)^{\\frac 1 \\gamma}$$\n",
    "\n",
    "and Generalised Riemann Invariant\n",
    "\n",
    "$$I_K(u, c_s) = u_K + \\frac{2 c_{sK}}{\\gamma - 1}$$\n",
    "\n",
    "which is constant across a rarefaction wave, so we can write,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  & u_L + \\frac{2 c_{sL}}{\\gamma - 1} = u_* + \\frac{2 c_{s*L}}{\\gamma - 1} \\\\\n",
    "  & u_* - \\frac{2 c_{s*R}}{\\gamma - 1} = u_R - \\frac{2 c_{sR}}{\\gamma - 1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where,\n",
    "\n",
    "$$\n",
    "c_{s*K} = c_{sK} \\left(\n",
    "  \\frac{p_*}{p_K}\n",
    "\\right)^{\\frac{\\gamma - 1}{2 \\gamma}}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
